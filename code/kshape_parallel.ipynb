{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import time \n",
    "from pyspark.mllib.linalg.distributed import *\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from numpy.linalg import norm, eigh\n",
    "from numpy.linalg import svd as SVD\n",
    "from numpy.fft import fft, ifft\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.linalg import DenseMatrix, DenseVector\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix,IndexedRowMatrix\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.mllib.linalg import DenseMatrix, DenseVector\n",
    "\n",
    "\n",
    "def zscore(a, axis=0, ddof=0):\n",
    "    # the same as python implementation\n",
    "    a = np.asanyarray(a)\n",
    "    mns = a.mean(axis=axis)\n",
    "    sstd = a.std(axis=axis, ddof=ddof)\n",
    "    if axis and mns.ndim < a.ndim:\n",
    "        res = ((a - np.expand_dims(mns, axis=axis)) /\n",
    "               np.expand_dims(sstd, axis=axis))\n",
    "    else:\n",
    "        res = (a - mns) / sstd\n",
    "    return np.nan_to_num(res)\n",
    "\n",
    "\n",
    "def roll_zeropad(a, shift, axis=None):\n",
    "    # the same as python implementation\n",
    "    a = np.asanyarray(a)\n",
    "    if shift == 0:\n",
    "        return a\n",
    "    if axis is None:\n",
    "        n = a.size\n",
    "        reshape = True\n",
    "    else:\n",
    "        n = a.shape[axis]\n",
    "        reshape = False\n",
    "    if np.abs(shift) > n:\n",
    "        res = np.zeros_like(a)\n",
    "    elif shift < 0:\n",
    "        shift += n\n",
    "        zeros = np.zeros_like(a.take(np.arange(n-shift), axis))\n",
    "        res = np.concatenate((a.take(np.arange(n-shift, n), axis), zeros), axis)\n",
    "    else:\n",
    "        zeros = np.zeros_like(a.take(np.arange(n-shift, n), axis))\n",
    "        res = np.concatenate((zeros, a.take(np.arange(n-shift), axis)), axis)\n",
    "    if reshape:\n",
    "        return res.reshape(a.shape)\n",
    "    else:\n",
    "        return res\n",
    "        for i in range(len(idx)):\n",
    "            dist = float('inf')\n",
    "            c = x[i]\n",
    "\n",
    "            for l,j in enumerate(centroids):\n",
    "                d,_ = _sbd(x[i],j)\n",
    "                if dist > d:\n",
    "                    c = j\n",
    "            idx[i] = l\n",
    "\n",
    "\n",
    "\n",
    "def _ncc_c(x, y):\n",
    "    # the same as python implementation    \n",
    "    den = np.array(norm(x) * norm(y))\n",
    "    den[den == 0] = np.Inf\n",
    "    x_len = len(x)\n",
    "    fft_size = 1 << (2*x_len-1).bit_length()\n",
    "    cc = ifft(fft(x, fft_size) * np.conj(fft(y, fft_size)))\n",
    "    cc = np.concatenate((cc[-(x_len-1):], cc[:x_len]))\n",
    "    return np.real(cc) / den\n",
    "\n",
    "def _ncc_c_3dim(x, y):\n",
    "\n",
    "    \"\"\"\n",
    "    Variant of NCCc that operates with 2 dimensional X arrays and 2 dimensional\n",
    "    y vector\n",
    "    Returns a 3 dimensional array of normalized fourier transforms\n",
    "    \"\"\"\n",
    "    den = norm(x, axis=1)[:, None] * norm(y, axis=1)\n",
    "    den[den == 0] = np.Inf\n",
    "    x_len = x.shape[-1]\n",
    "    fft_size = 1 << (2*x_len-1).bit_length()\n",
    "    cc = ifft(fft(x, fft_size) * np.conj(fft(y, fft_size))[:, None])\n",
    "    cc = np.concatenate((cc[:,:,-(x_len-1):], cc[:,:,:x_len]), axis=2)\n",
    "    return np.real(cc) / den.T[:, :, None]\n",
    "\n",
    "\n",
    "def _sbd(x, y):\n",
    "    # the same as python implementation\n",
    "    ncc = _ncc_c(x, y)\n",
    "    idx = ncc.argmax()\n",
    "    dist = 1 - ncc[idx]\n",
    "    yshift = roll_zeropad(y, (idx + 1) - max(len(x), len(y)))\n",
    "    return dist, yshift\n",
    "\n",
    "\n",
    "def _extract_shape(a,k):\n",
    "    # the same as python implementation\n",
    "    if len(a) == 0:\n",
    "        return np.zeros((1, k))\n",
    "\n",
    "        \n",
    "    a = np.vstack(a)    \n",
    "    columns = a.shape[1]\n",
    "    y = zscore(a, axis=1, ddof=1)\n",
    "    s = np.dot(y.transpose(), y)\n",
    "    p = np.empty((columns, columns))\n",
    "    p.fill(1.0/columns)\n",
    "    p = np.eye(columns) - p\n",
    "    m = np.dot(np.dot(p, s), p)\n",
    "    _, vec = eigh(m)\n",
    "    centroid = vec[:, -1]\n",
    "    finddistance1 = math.sqrt(((a[0] - centroid) ** 2).sum())\n",
    "    finddistance2 = math.sqrt(((a[0] + centroid) ** 2).sum())\n",
    "    if finddistance1 >= finddistance2:\n",
    "        centroid *= -1\n",
    "    return zscore(centroid, ddof=1)\n",
    "\n",
    "    \n",
    "\n",
    "# helper function that is applied to each partition during extract_shape step of each iterations\n",
    "# this function will first calc the closest centroid for each matrix and then consider this centroid as the current centroid in the extraction_shape step\n",
    "#produce the grammatian matrix needed produced from timeseries in that partition\n",
    "# then spark will aggregate these matrixes and calculate the new centroids    \n",
    "def find_shape(centroids):\n",
    "    def _find_shape(listOfSeries):\n",
    "        numClusters = centroids.shape[0]\n",
    "        feature_len = centroids.shape[1]\n",
    "        \n",
    "        cum_len = 0 \n",
    "        temp = []\n",
    "        for timeSeries in listOfSeries:\n",
    "            cum_len+=1\n",
    "            temp.append(timeSeries)\n",
    "        gramMatrix = [np.zeros(feature_len+1) for _ in range(cum_len)]\n",
    "        temp = np.vstack(temp)      \n",
    "        distances = (1 - _ncc_c_3dim(temp, centroids).max(axis=2)).T\n",
    "        idx = distances.argmin(1)\n",
    "        for i in range(cum_len): \n",
    "            closest = idx[i]\n",
    "            c = centroids[closest]\n",
    "            if np.count_nonzero(c) == 0:\n",
    "                series = temp[i]\n",
    "            else: \n",
    "                _, series = _sbd(c,temp[i])            \n",
    "            gramMatrix[i][0:-1] = series\n",
    "            gramMatrix[i][-1] = idx[i]\n",
    "                    \n",
    "        return gramMatrix\n",
    "    return _find_shape\n",
    "\n",
    "\n",
    "# similar to the above function, except that it is used in the first time of shape extraction\n",
    "# so it will use an initialized index of time series (currently i generate randomly, can later change it to the same one used in kmeans++)\n",
    "def find_shape1(centroids):\n",
    "    def _find_shape1(listOfSeries):\n",
    "        numClusters = centroids.shape[0]\n",
    "        feature_len = centroids.shape[1]\n",
    "        idx = []\n",
    "        temp = []\n",
    "        cum_len = 0\n",
    "        for timeSeries in listOfSeries:\n",
    "            cum_len+=1\n",
    "            temp.append(timeSeries[0:-1])\n",
    "            idx.append(int(timeSeries[-1]))\n",
    "        gramMatrix = [np.zeros(feature_len+1) for _ in range(cum_len)]   \n",
    "        for i in range(cum_len):      \n",
    "            (dist,xShift) = _sbd(centroids[idx[i]],temp[i])\n",
    "            gramMatrix[i][0:-1] = xShift\n",
    "            gramMatrix[i][-1] = idx[i]\n",
    "        return gramMatrix\n",
    "    return _find_shape1\n",
    "\n",
    "#freqyebt directiosn\n",
    "def fd2(ell,d,num_dict):\n",
    "    def _fd(timeseries):\n",
    "        res = [[] for i in range(num_dict)]\n",
    "        m = 2 * ell\n",
    "        temp = [[] for i in range(num_dict)]\n",
    "        for x in timeseries:\n",
    "            l = x.vector.toArray()\n",
    "            idx = int(l[-1])\n",
    "            temp[idx].append(l[0:-1])\n",
    "        \n",
    "        for idx,A in enumerate(temp):\n",
    "            sketch = np.zeros((m, d))\n",
    "            nextZeroRow = 0   \n",
    "            for vector in A:\n",
    "                if nextZeroRow >= m:\n",
    "                    [_,s,Vt] = SVD(sketch , full_matrices=False)                   \n",
    "                    if len(s) >= ell:\n",
    "                        sShrunk = np.sqrt(s[:ell]**2 - s[ell-1]**2)\n",
    "                        sketch[:ell:,:] = np.dot(np.diag(sShrunk), Vt[:ell,:])\n",
    "                        sketch[ell:,:] = 0\n",
    "                        nextZeroRow = ell\n",
    "                    else:\n",
    "                        sketch[:len(s),:] = np.dot(np.diag(s), Vt[:len(s),:])\n",
    "                        sketch[len(s):,:] = 0\n",
    "                        nextZeroRow = len(s)\n",
    "                sketch[nextZeroRow,:] = vector \n",
    "                nextZeroRow += 1      \n",
    "            [_,s,Vt] = SVD(sketch , full_matrices=False)\n",
    "            if len(s) >= ell:\n",
    "                    [_,s,Vt] = SVD(sketch , full_matrices=False)\n",
    "                    sShrunk = np.sqrt(s[:ell]**2 - s[ell-1]**2)\n",
    "                    sketch[:ell:,:] = np.dot(np.diag(sShrunk), Vt[:ell,:])\n",
    "                    sketch[ell:,:] = 0      \n",
    "            else:\n",
    "                    [_,s,Vt] = SVD(sketch , full_matrices=False)\n",
    "                    sketch[:len(s),:] = np.dot(np.diag(s), Vt[:len(s),:])\n",
    "                    sketch[len(s):,:] = 0\n",
    "            res[idx] = sketch[:ell,:]\n",
    "        return zip(range(num_dict),res[idx])\n",
    "    return _fd\n",
    "\n",
    "\n",
    "def kshape(ell,inputTimeSeries,numPartitions,num_dict, maxIters=1):\n",
    "    # explanation of input: k is length of each timesires, m is the number of timeseries,    \n",
    "    k = inputTimeSeries.first().shape[0]\n",
    "\n",
    "    \n",
    "    m = inputTimeSeries.count()\n",
    "    centroids = np.zeros((num_dict,k), dtype=float)\n",
    "    sc = inputTimeSeries.context\n",
    "    broadcast_numcluster = sc.broadcast(num_dict)\n",
    "    broadcast_centroids = sc.broadcast(centroids)\n",
    "    initialization = inputTimeSeries.map(lambda x: np.concatenate((x,[randint(0, broadcast_numcluster.value)])))   \n",
    "    p = np.empty((k, k))\n",
    "    p.fill(1.0/k)\n",
    "    p = np.eye(k) - p\n",
    "    p2 = np.zeros((k+1,k+1))\n",
    "    p2[0:-1,0:-1] = p\n",
    "    p2[-1][-1] = 1\n",
    "    Q = DenseMatrix(k+1,k+1,p2.T.flatten()) \n",
    "    #initialization, compute first set of centroids based on randomly assigned index\n",
    "    #returns each row as yshift against its centroid\n",
    "    X = initialization.\\\n",
    "            mapPartitions(find_shape1(broadcast_centroids.value)).zipWithIndex()\\\n",
    "            .map(lambda x: (x[1],x[0])).cache()\n",
    "    idx = X.map(lambda x: int(x[1][-1])).collect() \n",
    "    mat_X = IndexedRowMatrix(X)\n",
    "    mat_X = mat_X.multiply(Q) \n",
    "    Y = mat_X.rows.cache()\n",
    "    shapes = Y.mapPartitions(fd2(ell,k,num_dict)).reduceByKey(lambda a,b : a+b).\\\n",
    "            sortByKey().map(lambda x: x[1]).collect()\n",
    "    \n",
    "    new_centroids = np.vstack(shapes)    \n",
    "    centroids = new_centroids    \n",
    "    bcCentroids = sc.broadcast(centroids)\n",
    "    \n",
    "    for n in range(maxIters):\n",
    "        \n",
    "        start = time.time()\n",
    "        #reverses the original step \n",
    "        #calculate index first and then use it to extract\n",
    "        X = inputTimeSeries.\\\n",
    "            mapPartitions(find_shape(bcCentroids.value)).zipWithIndex().\\\n",
    "            map(lambda x: (x[1],x[0])).cache()\n",
    "        mat_X = IndexedRowMatrix(X)\n",
    "        mat_X = mat_X.multiply(Q)\n",
    "        Y = mat_X.rows.cache()\n",
    "        # use frequent direction to extract the new centroids \n",
    "        shapes = Y.mapPartitions(fd2(ell,k,num_dict)).reduceByKey(lambda a,b : a+b).\\\n",
    "                sortByKey().collect()\n",
    "            \n",
    "        new_centroids = np.vstack(shapes)          \n",
    "        new_idx = X.map(lambda x: int(x[1][-1])).collect()\n",
    "       \n",
    "        if np.array_equal(new_idx, idx):\n",
    "            print(n)\n",
    "            break\n",
    "        idx = new_idx\n",
    "        if n == maxIters-1:\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "        end = time.time() \n",
    "        bcCentroids = sc.broadcast(centroids)\n",
    "        #bcIndex = sc.broadcast(idx)\n",
    "    cluster_idx = [[] for _ in range(num_dict)]\n",
    "    print(new_idx)\n",
    "    for i in range(m):\n",
    "        cluster_idx[new_idx[i]].append(i+1)\n",
    "    clusters = []\n",
    "    \n",
    "    for i, centroid in enumerate(centroids):\n",
    "        clusters.append((centroid, cluster_idx[i]))\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n",
      "(2, 136)\n",
      "1231242\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 0.32953667, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101, -0.00244101, -0.00244101, -0.00244101, -0.00244101,\n",
       "        -0.00244101]),\n",
       " array([7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17,\n",
       "        7.72374203e-17, 7.72374203e-17, 7.72374203e-17, 7.72374203e-17])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the printed output is time elapsed \n",
    "#\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://yiyangou-VirtualBox:7077\")\\\n",
    "        .appName(\"kshape\")\\\n",
    "        .config(\"spark.cores.max\", \"1\")\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "label = []\n",
    "numPartitions = 6\n",
    "#lines = sc.textFile(\"SITS1M_TRAIN.csv\",16)\n",
    "testData = sc.textFile(\"./Desktop/GRAIL4Spark/ECGFiveDays.csv\",6).map(lambda x: np.array(list(map(float,x.split(\",\")))[0:-1])).cache()\n",
    "\n",
    "\n",
    "#testData = lines.map(lambda x: np.array(list(map(float,x.split(\",\")))))\n",
    "cluster_num=2\n",
    "ell = 25\n",
    "clusters = kshape(ell,testData,6, cluster_num)\n",
    "\n",
    "dic = list(map(lambda x: x[0],clusters))\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884\n",
      "kshape 0.7584429282730962\n",
      "kmeans 0.00034732532830314124\n"
     ]
    }
   ],
   "source": [
    "#compare kshape with kmeans\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from sklearn.cluster import KMeans as sk_kmeans \n",
    "\n",
    "\n",
    "kshape_label = [0 for _ in range(884)]\n",
    "print(len(kshape_label))\n",
    "for i in clusters[0][1]:\n",
    "    kshape_label[i] = 1\n",
    "for i in clusters[1][1]:\n",
    "    kshape_label[i] = 2\n",
    "    \n",
    "kmeans = sk_kmeans(n_clusters=2, random_state=0).fit(content)\n",
    "kmeans_label= kmeans.predict(content)\n",
    "\n",
    "print(\"kshape \" +str(adjusted_rand_score(label,kshape_label)))\n",
    "\n",
    "print(\"kmeans \" +str(adjusted_rand_score(label,kmeans_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884\n"
     ]
    }
   ],
   "source": [
    "def find_shape1(centroids):\n",
    "    def _find_shape1(listOfSeries):\n",
    "        numClusters = centroids.shape[0]\n",
    "        feature_len = centroids.shape[1]\n",
    "        idx = []\n",
    "        temp = []\n",
    "        cum_len = 0\n",
    "        for timeSeries in listOfSeries:\n",
    "            cum_len+=1\n",
    "            temp.append(timeSeries[0:-1])\n",
    "            idx.append(int(timeSeries[-1]))\n",
    "        gramMatrix = [np.zeros(feature_len+1) for _ in range(cum_len)]   \n",
    "        for i in range(cum_len):      \n",
    "            (dist,xShift) = _sbd(centroids[idx[i]],temp[i])\n",
    "            gramMatrix[i][0:-1] = xShift\n",
    "            gramMatrix[i][-1] = idx[i]\n",
    "        return gramMatrix\n",
    "    return _find_shape1\n",
    "\n",
    "\n",
    "inputTimeSeries = testData.map(lambda x: np.concatenate((x,[randint(0, cluster_num)])))\\\n",
    "                                          .cache()   \n",
    "inputTimeSeries.count()\n",
    "centroids = np.zeros((cluster_num,136), dtype=float)\n",
    "X = inputTimeSeries.\\\n",
    "    mapPartitions(find_shape1(centroids)).count()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randint(0, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
