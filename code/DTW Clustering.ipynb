{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import utils\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from numpy.random import permutation as rpm\n",
    "from plot import plot_images, plot_confusion_matrix\n",
    "from utils import resize, sample_index, gen_solution, get_purity, get_nmi\n",
    "from dtw import dtw\n",
    "from numpy.linalg import norm\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.linalg import DenseMatrix, DenseVector\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix,IndexedRowMatrix\n",
    "from pyspark.mllib.linalg import DenseMatrix, DenseVector\n",
    "from fastdtw import *\n",
    "from time import time\n",
    "import json\n",
    "import csv\n",
    "import itertools\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://10.140.81.48:7077\")\\\n",
    "        .appName(\"DTW\")\\\n",
    "        .config(\"spark.executor.memory\",\"10g\")\\\n",
    "        .config(\"spark.executor.cores\",\"5\")\\\n",
    "        .config(\"spark.driver.memory\",\"20G\")\\\n",
    "        .config(\"spark.cores.max\", \"80\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "numPartitions=160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json1_file = open('pad_swbd_train_.json')\n",
    "json1_str = json1_file.read()\n",
    "train_data = json.loads(json1_str)\n",
    "\n",
    "json1_file = open('pad_swbd_dev_.json')\n",
    "json1_str = json1_file.read()\n",
    "dev_data = json.loads(json1_str)\n",
    "\n",
    "json1_file = open('pad_swbd_test_.json')\n",
    "json1_str = json1_file.read()\n",
    "test_data = json.loads(json1_str)\n",
    "\n",
    "json1_data = train_data[\"data\"] + dev_data[\"data\"] + test_data[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015748"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#learn pca\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "X = sc.parallelize(json1_data,numPartitions).cache()\n",
    "X.count()\n",
    "mat = RowMatrix(X)\n",
    "pc = mat.computePrincipalComponents(10)\n",
    "projected = mat.multiply(pc)\n",
    "\n",
    "ca = projected.rows.cache()\n",
    "ca.count()\n",
    "\n",
    "#recombine\n",
    "data = ca.collect()\n",
    "frames = train_data[\"frames\"]+dev_data[\"frames\"] +test_data[\"frames\"]\n",
    "t_data = []\n",
    "prev = 0\n",
    "temp = []\n",
    "for i in range(len(data)):\n",
    "    a, b = frames[i]\n",
    "    if a == prev: \n",
    "        temp.append(data[i].toArray())\n",
    "    else: \n",
    "        prev = a \n",
    "        t_data.append(np.array(temp))\n",
    "        temp = []\n",
    "        temp.append(data[i].toArray())\n",
    "t_data.append(np.array(temp))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of signals: 31961\n"
     ]
    }
   ],
   "source": [
    "# load padded data\n",
    "data=np.load('data/swbd.npy').item()\n",
    "train_x = data['train']['data']\n",
    "train_y_raw = train_data[\"labels\"]\n",
    "train_y = [label.split('_')[0] for label in train_y_raw]\n",
    "dev_x = data['dev']['data']\n",
    "dev_y_raw = dev_data[\"labels\"]\n",
    "dev_y = [label.split('_')[0] for label in dev_y_raw]\n",
    "test_x = data['test']['data']\n",
    "test_y_raw = test_data[\"labels\"]  \n",
    "test_y = [label.split('_')[0] for label in test_y_raw]\n",
    "\n",
    "# used in clustering\n",
    "signals = t_data # tuple of numpy.ndarray  31961 * [time*39]\n",
    "labels = train_y+dev_y+test_y  # labels. tuple of strings 31961. e.g. 'abandoned'\n",
    "\n",
    "print('number of signals: {}'.format(len(signals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of words is 6204\n"
     ]
    }
   ],
   "source": [
    "# count number of words. Change strings to numeric labels\n",
    "wordSet = set()\n",
    "for word in labels:\n",
    "    wordSet.add(word)\n",
    "print('total number of words is {}'.format(len(wordSet)))\n",
    "\n",
    "# make a list. let list index be the numeric label of word\n",
    "wordList = list(wordSet)\n",
    "\n",
    "# numeric label of signals\n",
    "numLabels = np.array(list(map(lambda x: wordList.index(x), labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Embedding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embeddings: (31961, 512)\n"
     ]
    }
   ],
   "source": [
    "# load embedding data\n",
    "emb_data = np.load('data/swbd_embeddings.npy').item()\n",
    "emb_train_x = emb_data['train']['embs']\n",
    "emb_train_y_raw = emb_data['train']['labels']\n",
    "emb_train_y = [label.split('_')[0] for label in train_y_raw]\n",
    "emb_dev_x = emb_data['dev']['embs']\n",
    "emb_dev_y_raw = emb_data['dev']['labels']\n",
    "emb_dev_y = [label.split('_')[0] for label in dev_y_raw]\n",
    "emb_test_x = emb_data['test']['embs']\n",
    "emb_test_y_raw = emb_data['test']['labels']\n",
    "emb_test_y = [label.split('_')[0] for label in test_y_raw]\n",
    "\n",
    "# labels are the same. Use the embedding matrix in clustering\n",
    "emb_signals = np.concatenate((emb_train_x,emb_dev_x,emb_test_x), axis = 0) # np.ndarray 31961*512\n",
    "print('shape of embeddings: {}'.format(emb_signals.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small raw data and small labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('because', 340), ('recycling', 315), ('benefits', 259), ('something', 230), ('exactly', 228), ('probably', 225), ('insurance', 196), ('punishment', 190), ('everything', 174), ('company', 149), ('sometimes', 144), ('interesting', 143), ('recycle', 143), ('situation', 141), ('problem', 139), ('anything', 131), ('plastic', 127), ('actually', 125), ('understand', 123), ('vacation', 123)]\n",
      "total number of series: 3645\n"
     ]
    }
   ],
   "source": [
    "class_num = 20\n",
    "wordBags = dict()\n",
    "for word in labels:\n",
    "    if word in wordBags:\n",
    "        wordBags[word] += 1\n",
    "    else:\n",
    "        wordBags[word] = 1\n",
    "sorted_by_value = sorted(wordBags.items(), key=lambda kv: -kv[1])\n",
    "print(sorted_by_value[:class_num])\n",
    "print('total number of series: {}'.format(sum([x[1] for x in sorted_by_value[:class_num]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the small data set\n",
    "small_index_list = [x[0] for x in sorted_by_value[:class_num]]\n",
    "small_raw_signal = []\n",
    "small_emb_data = []\n",
    "small_num_labels = []\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] in small_index_list:\n",
    "        small_raw_signal.append(signals[i])\n",
    "        small_emb_data.append(emb_signals[i,:])\n",
    "        small_num_labels.append(small_index_list.index(labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinate the embeded signal to make a data matrix\n",
    "small_emb_signal = np.zeros((len(small_emb_data), 512))\n",
    "for i in range(len(small_emb_data)):\n",
    "    small_emb_signal[i] = small_emb_data[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small data summary: \n",
    "1. small_index_list: array with length 20. Used as a reference for numeric label\n",
    "\n",
    "    ['because', 'recycling', 'benefits', 'something', 'exactly', 'probably', \n",
    "    'insurance', 'punishment', 'everything', 'company', 'sometimes',\n",
    "    'interesting', 'recycle', 'situation', 'problem', 'anything', \n",
    "    'plastic', 'actually', 'understand', 'vacation']\n",
    "                 \n",
    "                 \n",
    "2. small_raw_signal: # list of numpy.ndarray  3645 * [time*39]\n",
    "\n",
    "3. small_emb_signal: # numpy.ndarray 3645 * 512\n",
    "\n",
    "4. small_num_labels: # array with length 3645. Numeric Label for each signal.\n",
    "\n",
    "The signals of embedding coordinate with signals of raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 60)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_raw_signal[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with DTW distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original\n",
    "\n",
    "start = time()\n",
    "def wrapper_dtw(data):\n",
    "    def _dtw(x_tup):\n",
    "        x = x_tup[1]\n",
    "        idx = x_tup[0]\n",
    "        res = np.zeros(len(data))\n",
    "        start = time()\n",
    "        for i in range(idx,len(data)):    \n",
    "            res[i] = fastdtw(x,data[i],dist=lambda x,y: norm(x-y))[0]\n",
    "        print(time()-start)\n",
    "        return (idx,res)\n",
    "    return _dtw\n",
    "\n",
    "n_data = len(small_num_labels)\n",
    "Euc_Lap = np.zeros((n_data,n_data))\n",
    "zip_small_raw_signal = list(zip(range(len(small_raw_signal)),small_raw_signal))\n",
    "distributed_raw = sc.parallelize(zip_small_raw_signal,numPartitions).coalesce(numPartitions,shuffle=True).cache()\n",
    "start = time()\n",
    "\n",
    "sc_data = sc.broadcast(small_raw_signal)\n",
    "tempo = distributed_raw.map(wrapper_dtw(sc_data.value)).cache() \n",
    "print(tempo.count())\n",
    "Euc_Lap = tempo.sortByKey(lambda x: x[0]).map(lambda x: x[1]).collect()\n",
    "\n",
    "\n",
    "\n",
    "Euc_Lap2 = Euc_Lap\n",
    "for i in range(len(test_labels)):\n",
    "    for j in range(i):\n",
    "        Euc_Lap2[i][j] = Euc_Lap[j][i]\n",
    "\"\"\"\n",
    "Hierarchical clustering of embedding data using distance matrix\n",
    "\"\"\"\n",
    "from clustering import kmeans_cluster, hierarchical_cluster, hierarchical_cluster_with_distMat\n",
    "linkage_matrix, pred = hierarchical_cluster_with_distMat(np.array(Euc_Lap2), n_clusters=len(small_index_list),\n",
    "                                                         method='ward', metric='cosine')\n",
    "_, cm, purity = get_purity(pred, small_num_labels)\n",
    "nmi = get_nmi(pred, small_num_labels)\n",
    "plot_confusion_matrix(cm, small_index_list, normalize=False, rotation=90, figsize=(9, 9))\n",
    "print(\"Ward, NMI: %.3f, Purity: %.3f\" % (nmi, purity))\n",
    "\n",
    "''' \n",
    "pred = clustering.labels_\n",
    "_, cm, purity = get_purity(pred, small_num_labels)\n",
    "nmi = get_nmi(pred, small_num_labels)\n",
    "plot_confusion_matrix(cm, small_index_list, normalize=False, rotation=90, figsize=(9, 9))\n",
    "print(\"rbf, NMI: %.3f, Purity: %.3f\" % (nmi, purity))\n",
    "end = time.time()\n",
    "print('time used: {}'.format(end-start))\n",
    "'''\n",
    "print(time()-start)\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "#rand = np.random.randint(0,20,61)\n",
    "#print(adjusted_rand_score(rand,small_num_labels))\n",
    "print(adjusted_rand_score(pred,small_num_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
